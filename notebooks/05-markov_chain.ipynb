{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Markov chain Monte Carlo\n",
    "\n",
    "- A different approach to sample distributions is based on the construction\n",
    "  of a Markov chain.\n",
    "  \n",
    "- It will turn out to be a very powerful approach.\n",
    "\n",
    "- It will also introduce some difficulties. In particular, the samples that will\n",
    "  be generated are *no longer going to be independent*.\n",
    "  \n",
    "- We will therefore have to be careful when estimating error bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.style.use('style.mpl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Markov chain and transition probability\n",
    "\n",
    "- A Markov chain is a sequence of elements $x_1 \\to x_2 \\to x_3 \\to x_4 \\to \\ldots$\n",
    "  generated by a Markov process.\n",
    "  \n",
    "- It is characterized by a *transition probability* $p(x \\to y)$. It is normalized\n",
    "  $\\sum_y p(x \\to y) = 1$.\n",
    "\n",
    "- During the Markov process, a new element $x_{n+1}$ is added to the chain with\n",
    "  a probability $p(x_n \\to x_{n+1})$.\n",
    "\n",
    "- This transition probability only depends on the previous element, there is\n",
    "  no memory of the distant past of the chain.\n",
    "  \n",
    "- We will also suppose *ergodicity*, i.e. there is a non-zero probability path\n",
    "  between any two $x$ and $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Equilibrium probability distribution\n",
    "\n",
    "- What is the probability to be in a given $x$ at step $n+1$?\n",
    "\n",
    "  \\begin{align*}\n",
    "  \\pi_{n+1} (x) &= \\sum_{y \\ne x} \\pi_n(y) \\, p(y \\to x) + \\pi_n(x) \\, p(x \\to x) \\\\\n",
    "  &= \\sum_{y \\ne x} \\pi_n(y) \\, p(y \\to x) + \\pi_n(x) \\, \\big( 1 - \\sum_{y \\ne x} p(x \\to y) \\big) \n",
    "  \\end{align*}\n",
    "  \n",
    "- Suppose the distribution becomes stationary $\\pi(x) = \\pi_{n+1}(x) = \\pi_n(x)$\n",
    "  \n",
    "- Simplifying the equation above we obtain\n",
    "\n",
    "  \\begin{equation*}\n",
    "  \\sum_{y \\ne x} \\pi(x) \\, p(x \\to y) = \\sum_{y \\ne x} \\pi(y) \\, p(y \\to x)\n",
    "  \\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Global and detailed balance\n",
    "\n",
    "- Let's say we want to sample a distribution $\\pi(x)$ using a Markov chain Monte Carlo.\n",
    "\n",
    "- It is enough to find a transition probability $p(x \\to y)$ that satisfies\n",
    "  the **global balance condition**\n",
    "\n",
    "  \\begin{equation*}\n",
    "  \\boxed{\n",
    "  \\sum_{y} \\pi(x) \\, p(x \\to y) = \\sum_{y} \\pi(y) \\, p(y \\to x)}\n",
    "  \\end{equation*}\n",
    "\n",
    "- The condition above is necessary, but it is often simpler to design transition\n",
    "  probabilities that satisfy the stronger **detailed balance condition**\n",
    "  \n",
    "  \\begin{equation*}\n",
    "  \\boxed{\n",
    "  \\pi(x) \\, p(x \\to y) = \\pi(y) \\, p(y \\to x)}\n",
    "  \\end{equation*}\n",
    "\n",
    "- The game is now to find a recipe to find $p(x \\to y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: the pebble game\n",
    "\n",
    "- Consider a $3 \\times 3$ tiling. A pebble is sitting on one of the\n",
    "  tiles $k \\in \\{0, \\ldots, 8\\}$.\n",
    "\n",
    "- We want to move the pebble to one of its neighbors with a transition\n",
    "  probability $p(x \\to y)$ such that the pebble will have a uniform\n",
    "  distribution over the tiling $\\pi_k = 1/9$.\n",
    "  \n",
    "- The detailed balance imposes $p(x \\to y) = p(y \\to x)$.\n",
    "\n",
    "&nbsp;\n",
    "<center>\n",
    "<img src=\"figures/cells.png\" alt=\"Drawing\" style=\"height: 200px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We can use the following transition rules:\n",
    "\n",
    "  - The probability to move to a neighbor is $1/4$. This ensures that\n",
    "    $p(x \\to y) = p(y \\to x)$.\n",
    "    \n",
    "  - If there are less than 4 neighbors (edge, corner) the pebble can\n",
    "    stay on its tile with the remaining probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "L = 3\n",
    "n_samples = 2**14\n",
    "samples = np.zeros(n_samples, dtype=int)\n",
    "\n",
    "for i in range(n_samples-1):\n",
    "    \n",
    "    # k -> (x,y)\n",
    "    coord = np.unravel_index(samples[i], [L,L])\n",
    "    \n",
    "    # pick random shift (prob 1/4)\n",
    "    ind = np.random.randint(2)\n",
    "    shift = np.zeros(2, dtype=int)\n",
    "    shift[ind] = np.random.choice([-1,1])\n",
    "    \n",
    "    # new (x,y) -> clip inside square -> k\n",
    "    coord += shift\n",
    "    samples[i+1] = np.ravel_multi_index(coord, [L,L], mode='clip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "to_remove"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(samples, bins=np.arange(-0.5, L**2+0.5), density=True, ec='w', alpha=0.7, label='sampled')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "ax.set_xlabel(\"tile $k$\")\n",
    "ax.set_ylabel(\"$\\pi_k$\");\n",
    "ax.set_ylim(0, 0.15)\n",
    "ax.set_title(\"Pebble game\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "- Markov chain Monte Carlo (MCMC) is a different way to generate samples $x_i$\n",
    "  that are distributed according to some probability $\\pi$.\n",
    "  \n",
    "- A desired probability $\\pi$ is obtained by choosing the transition probability\n",
    "  such that it satifies the global balance condition\n",
    "\n",
    "  \\begin{equation*}\n",
    "  \\sum_{y} \\pi(x) \\, p(x \\to y) = \\sum_{y} \\pi(y) \\, p(y \\to x)\n",
    "  \\end{equation*}\n",
    "  \n",
    "  or the detailed balance condition\n",
    "  \n",
    "  \\begin{equation*}\n",
    "  \\pi(x) \\, p(x \\to y) = \\pi(y) \\, p(y \\to x)\n",
    "  \\end{equation*}\n",
    "\n",
    "- Finding a good transition probability is not trivial. If we change the rules of\n",
    "  the pebble game and ask that the probability to sit on the tile $k$ is some $\\pi_k$. What\n",
    "  should I choose for $p(x \\to y)$?\n",
    "  \n",
    "- MCMC algorithms provide recipes to find $p(x \\to y)$. There are many of them, e.g.\n",
    "  the Glauber, Wolff, heat bath algorithms, etc. We will focus on the\n",
    "  **Metropolis-Hastings** algorithm.\n",
    "\n",
    "- Unlike direct sampling, Markov chain sampling generates *correlated*\n",
    "  samples. We will have to keep this in mind!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Outline\n",
    "    \n",
    "- [Introduction to Monte Carlo](01-intro_monte_carlo.slides.html)\n",
    "\n",
    "- [Newtonâ€“Cotes quadrature](02-newton_cotes.slides.html)\n",
    "  \n",
    "- [Importance sampling](03-importance_sampling.slides.html)\n",
    "\n",
    "- [Direct sampling methods](04-direct_sampling.slides.html)\n",
    "\n",
    "- [Markov chain sampling and balance condition](05-markov_chain.slides.html)\n",
    "\n",
    "- [Metropolis-Hastings algorithm](06-metropolis_hastings.slides.html)\n",
    "\n",
    "- [The two-dimensional Ising model](07-ising.slides.html)\n",
    "\n",
    "- [Error bar analysis](08-error_analysis.slides.html)\n",
    "  \n",
    "- [References](09-references.slides.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
