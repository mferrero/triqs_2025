{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Error bar analysis\n",
    "\n",
    "- In Markov chain Monte Carlo algorithms, the sampled outputs are generally\n",
    "  correlated.\n",
    "  \n",
    "- This has important consequences is the way to quantify the statistical\n",
    "  error on the results.\n",
    "  \n",
    "- We will take a Markov chain from the Ising model and use it to assess different\n",
    "  strategies to compute error bars.\n",
    "  \n",
    "- But before that, let us remember some results for independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(82373)\n",
    "plt.style.use('style.mpl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Independent random variables\n",
    "\n",
    "- Imagine we have a set of $N$ *independent* random variables $\\{ x_1, x_2, \\ldots, x_N \\}$\n",
    "  and they all have the same mean $\\mu$ and variance $\\sigma^2$.\n",
    "  \n",
    "- We introduce the *empirical average*\n",
    "\n",
    "  \\begin{equation*}\n",
    "    X = \\frac{1}{N} \\sum_{i=1}^N x_i\n",
    "  \\end{equation*}\n",
    "  \n",
    "- Even for finite $N$, we have\n",
    "\n",
    "  \\begin{equation*}\n",
    "  \\langle X \\rangle = \\frac{1}{N} \\sum_{i=1}^N \\langle x_i \\rangle = \\mu\n",
    "  \\end{equation*}\n",
    "  \n",
    "- This means that from a finite sample, the *best estimate* for the true average\n",
    "  value is the sample average.\n",
    "  \n",
    "- We say that the sample average is an **unbiased estimator**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Standard deviation of the empirical average\n",
    "\n",
    "- The variance of the empirical average is\n",
    "\n",
    "  \\begin{equation*}\n",
    "  \\sigma_X^2 = \\langle X^2 \\rangle - \\langle X \\rangle^2 = \\frac{1}{N^2} \\sum_{i,j=1}^N \\langle x_i x_j\n",
    "  \\rangle  - \\frac{1}{N^2} \\sum_{i,j=1}^N \\langle x_i \\rangle \\langle x_j \\rangle = \\frac{\\sigma^2}{N}\n",
    "  \\end{equation*}\n",
    "  \n",
    "- This is a useful theoretical result, it tells us how the error bar decreases with $N$. But\n",
    "  we do not know $\\sigma$. How do we estimate $\\sigma_X^2$ from our data?\n",
    "  \n",
    "- It is natural to start from the sample variance\n",
    "\n",
    "  \\begin{equation*}\n",
    "    s^2 = \\frac{1}{N} \\sum_{i=1}^N \\big( x_i - \\frac{1}{N} \\sum_{j=1}^N x_j \\big)^2\n",
    "    = \\frac{1}{N} \\sum_{i=1}^N x_i^2 - \\frac{1}{N^2} \\sum_{i,j=1}^N x_i x_j\n",
    "  \\end{equation*}\n",
    "  \n",
    "- From it's average value, we can design the *best estimate* for the variance of the\n",
    "  empirical average\n",
    "\n",
    "  \\begin{equation*}\n",
    "  \\langle s^2 \\rangle = \\langle x_i^2 \\rangle - \\frac{(N-1)}{N} \\langle x_i\n",
    "  \\rangle^2 - \\frac{1}{N} \\langle x_i^2 \\rangle = \\frac{N-1}{N} \\sigma^2\n",
    "  \\quad \\to \\quad\n",
    "  \\boxed{\n",
    "  \\sigma_X^2 \\simeq \\frac{s^2}{N-1}}\n",
    "  \\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Biased vs unbiased estimators\n",
    "\n",
    "- In the calculation above, we have an example where the average of an empirical estimator\n",
    "  (e.g. the sample variance) is not the actual average:\n",
    "  \n",
    "  \\begin{equation*}\n",
    "    \\sigma^2 = \\frac{N}{N-1} \\langle s^2 \\rangle = \\langle s^2 \\rangle + \\frac{1}{N} \\langle s^2 \\rangle + \\ldots\n",
    "  \\end{equation*}\n",
    "  \n",
    "- Such estimators are called **biased estimators**.\n",
    "\n",
    "- To obtain the best estimate from the sample average, one should add corrections\n",
    "  starting at order $1/N$.\n",
    "  \n",
    "- In general, this happens for non-linear functions $f$ of averages\n",
    "  \n",
    "  \\begin{equation*}\n",
    "  f(\\langle x \\rangle, \\langle y \\rangle, \\ldots) \\simeq f(X, Y, \\ldots) + \\mathcal{O}(\\frac{1}{N})\n",
    "  \\end{equation*}\n",
    "  \n",
    "  where $f(X,Y,\\ldots)$ is the function evaluated at the sample averages $X$, $Y$, ...\n",
    "\n",
    "- However, in practice, because the standard deviation of the empirical average goes as\n",
    "  $1/\\sqrt{N}$, the corrections to the biased estimators can be neglected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# An actual Markov chain from the Ising model\n",
    "\n",
    "- Let us generate a Markov chain for the energy in the Ising model for\n",
    "  $T = 3$ on an $8 \\times 8$ lattice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "def initialize(L):\n",
    "    σ = np.random.choice([-1,1], size=(L,L))\n",
    "    return σ\n",
    "\n",
    "def monte_carlo_step(σ, β):\n",
    "    \n",
    "    L = σ.shape[0]\n",
    "    k, l = np.random.randint(L, size=2) # pick a site   \n",
    "    ΔE = 2 * σ[k,l] * (σ[(k+1)%L,l] + σ[(k-1)%L,l] + σ[k,(l+1)%L] + σ[k,(l-1)%L])\n",
    "\n",
    "    # accept the flip\n",
    "    if np.random.rand() < np.exp(-β * ΔE):\n",
    "        σ[k,l] *= -1\n",
    "            \n",
    "    return σ\n",
    "\n",
    "def compute_energy(σ):\n",
    "    L = σ.shape[0]\n",
    "    E = 0\n",
    "    for k in range(L):\n",
    "        for l in range(L):\n",
    "            E -= σ[k,l] * (σ[(k+1)%L,l] + σ[k,(l+1)%L])\n",
    "    return E / L**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# A single Monte Carlo run\n",
    "n_samples = 2**16\n",
    "n_warmup = 1000\n",
    "energies = np.zeros(n_samples)\n",
    "\n",
    "L = 8\n",
    "T = 3.0\n",
    "β = 1 / T\n",
    "σ = initialize(L)\n",
    "\n",
    "# MC simulation\n",
    "for k in range(n_samples + n_warmup):\n",
    "    monte_carlo_step(σ, β)\n",
    "    if k >= n_warmup:\n",
    "        energies[k-n_warmup] = compute_energy(σ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "to_remove"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(8,8))\n",
    "\n",
    "ax[0].plot(energies, alpha=0.4, label=\"configuration\")\n",
    "ax[0].plot(np.cumsum(energies) / np.arange(1,n_samples+1), '-r', label=\"running average\")\n",
    "ax[0].set_title(\"Energy series\")\n",
    "ax[0].set_xlabel(\"MC steps\")\n",
    "ax[0].set_ylabel(\"$E$\")\n",
    "ax[0].legend(frameon=True, framealpha=0.95)\n",
    "\n",
    "ax[1].plot(energies, alpha=0.4, label=\"configuration\")\n",
    "ax[1].plot(np.cumsum(energies) / np.arange(1,n_samples+1), '-r', label=\"running average\")\n",
    "ax[1].set_title(\"Energy series\")\n",
    "ax[1].set_xlabel(\"MC steps\")\n",
    "ax[1].set_ylabel(\"$E$\")\n",
    "ax[1].legend(frameon=True, framealpha=0.95)\n",
    "ax[1].set_xlim(0, 5000)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- By generating several independent Markov chains, we can estimate the actual error bar on\n",
    "  the empirical average.\n",
    "  \n",
    "- This error bar is much larger than the one from the naive formula for independent variables.\n",
    "\n",
    "- We can infer an *autocorrelation time $\\tau$* which gives an idea of how many steps\n",
    "  are needed for two measurements to be independent\n",
    "\n",
    "  \\begin{equation*}\n",
    "    2 \\tau = \\frac{\\sigma^2_\\mathrm{correct}}{\\sigma^2_\\mathrm{naive}}\n",
    "  \\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "to_remove"
    ]
   },
   "outputs": [],
   "source": [
    "# Let's produce several Markov chains and measure the error bar\n",
    "n_samples = 2**16\n",
    "n_warmup = 1000\n",
    "n_chains = 40\n",
    "\n",
    "L = 8\n",
    "T = 3.0\n",
    "β = 1 / T\n",
    "\n",
    "energies = np.zeros([n_samples, n_chains])\n",
    "for i in range(n_chains):\n",
    "    \n",
    "    # MC simulation\n",
    "    σ = initialize(L)\n",
    "    for k in range(n_samples + n_warmup):\n",
    "        monte_carlo_step(σ, β)\n",
    "        if k >= n_warmup:\n",
    "            energies[k-n_warmup,i] = compute_energy(σ)\n",
    "            \n",
    "averages = np.average(energies, axis=0)\n",
    "print(f\"Average energies for {n_chains} Markov chains\")\n",
    "print(averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "to_remove"
    ]
   },
   "outputs": [],
   "source": [
    "error_actual = np.std(averages)\n",
    "print(f\"Actual error bar: {error_actual:.5f}\")\n",
    "print(f\"Naive calculation: {np.std(energies) / np.sqrt(n_samples-1):.5f}\")\n",
    "print(f\"Autocorrlation time τ: {n_samples * np.var(averages)  / np.var(energies) / 2:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Correlated random variables\n",
    "\n",
    "- Even for correlated variables, the empirical average is the best estimate of the true average\n",
    "\n",
    "- An estimate of the variance of the empirical average is obtained from\n",
    "\n",
    "  \\begin{equation*}\n",
    "    \\sigma_X^2 = \\frac{s^2}{N-1} \\, 2 \\tau\n",
    "  \\end{equation*}\n",
    "  \n",
    "  where the autocorrelation time $\\tau$ is estimated as\n",
    "  \n",
    "  \\begin{equation*}\n",
    "    \\tau = \\frac{1}{2} + \\sum_{k=1}^N A(k)\n",
    "    \\qquad\n",
    "    A(k) = \\frac{\\langle x_1 x_{1+k} \\rangle - \\langle x_1 \\rangle \\langle x_{1+k} \\rangle}{s^2}\n",
    "  \\end{equation*}\n",
    "  \n",
    "- Drawback: one needs to evaluate the autocorrelation function $A$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "to_remove"
    ]
   },
   "outputs": [],
   "source": [
    "# compute autocorrelation function for all chains\n",
    "auto = np.zeros([n_samples, n_chains])\n",
    "for i in range(n_chains):\n",
    "    shifted = energies[:,i] - np.average(energies[:,i])\n",
    "    auto[:,i] = np.correlate(shifted, shifted, \"full\")[n_samples-1:]\n",
    "    auto[:,i] /= auto[0,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "to_remove"
    ]
   },
   "outputs": [],
   "source": [
    "plt.plot(auto, alpha=0.1, c=\"C1\")\n",
    "plt.plot(np.average(auto, axis=1), 'r-', lw=3)\n",
    "plt.axhline(0, linestyle=\"--\")\n",
    "plt.xlim(0, 1500)\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.title(\"Autocorrelation function\")\n",
    "plt.xlabel(\"Time $k$\")\n",
    "plt.ylabel(\"$A(k)$\")\n",
    "\n",
    "τ = 0.5 + np.sum(np.average(auto, axis=1)[0:1500])\n",
    "error_auto = np.std(energies[:,0]) * np.sqrt(2 * τ / (n_samples-1))\n",
    "\n",
    "print(f\"Actual error bar: {error_actual:.5f}\")\n",
    "print(f\"Error bar with autocorrelation: {error_auto:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Binning analysis\n",
    "\n",
    "- A versatile approach to estimate error bar is the *binning analysis*.\n",
    "\n",
    "- The idea is to construct a new series of samples by taking averages of pairs of\n",
    "  consecutive samples of the original series.\n",
    "  \n",
    "  \\begin{equation*}\n",
    "    A^{(\\ell)}_i = \\frac{1}{2} \\left( A^{(\\ell-1)}_{2i-1} + A^{(\\ell-1)}_{2i} \\right)\n",
    "  \\end{equation*}\n",
    "  \n",
    "- The new series is twice shorter. We can estimate the variance of this new series.\n",
    "  and then continue like this.\n",
    "\n",
    "- If the bins are long enough we can expect that they become independant and that\n",
    "  we will converge to the correct value.\n",
    "  \n",
    "- Only drawback: one may need a very large number of measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "to_remove"
    ]
   },
   "outputs": [],
   "source": [
    "error = np.zeros([16, n_chains])\n",
    "binned = energies.copy()\n",
    "for i in range(16):\n",
    "    error[i,:] = np.std(binned, axis=0) / np.sqrt(binned.shape[0]-1)\n",
    "    binned = np.average(binned.reshape(-1,2,n_chains), axis=1)\n",
    "    \n",
    "#plt.plot(error, alpha=0.1, color='C1');\n",
    "plt.plot(np.average(error, axis=1), '-o', label=\"Binning\");\n",
    "plt.axhline(error_actual, color='red', linestyle='--', label=\"Correct\")\n",
    "plt.axhline(error_auto, color='blue', linestyle='--', label=\"Autocorrelation\")\n",
    "plt.ylim(0,0.025)\n",
    "plt.title(\"Binning analysis\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"$\\log_2$(bin size)\")\n",
    "plt.ylabel(\"Standard deviation\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Error propagation\n",
    "\n",
    "- We have discussed two ways to obtain the error bar of a sample average.\n",
    "\n",
    "- How do we estimate the error bar of\n",
    "  $\\mathcal{A} = f(\\langle x \\rangle, \\langle y \\rangle)$, a function of averages,\n",
    "  e.g. the specific heat or Binder cumulant?\n",
    "  \n",
    "  \\begin{equation*}\n",
    "    C_V \\left(\\langle e \\rangle, \\langle e^2 \\rangle \\right) = \\frac{1}{T^2} \\left( \\langle e^2 \\rangle - \\langle e \\rangle^2 \\right) \\qquad\n",
    "    B_4 \\left(\\langle m^2 \\rangle, \\langle m^4 \\rangle \\right) = \\frac{\\langle m^4 \\rangle}{3\\langle m^2 \\rangle^2}\n",
    "  \\end{equation*}\n",
    "\n",
    "- Let's imagine we have produced the sample averages $X$, $Y$. We can estimate \n",
    "  $\\mathcal{A} \\simeq f(X,Y)$. This estimator may be biased, but the correction is in $1/N$\n",
    "  and can be neglected if $N$ is large.\n",
    "  \n",
    "- In order to estimate the error bar, a first possibility is to do the usual *error propagation*\n",
    "  from the knowledge of the error bar on $X$, $Y$ (obtained e.g. by binning).\n",
    "  \n",
    "- This can become cumbersome and is not very generic. A useful and versatile alternative\n",
    "  approach is the *Jackknife analysis*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Jackknife analysis\n",
    "\n",
    "- Start by cutting the measurement series into $M$ bins.\n",
    "\n",
    "- For every bin compute the sample averages $X_i$, $Y_i$. Then we introduce\n",
    "  the Jackknife averages\n",
    "  \n",
    "  \\begin{equation*}\n",
    "    X^J_j = \\frac{1}{M-1} \\sum_{i\\ne j} X_j \\qquad Y^J_j = \\frac{1}{M-1} \\sum_{i\\ne j} Y_j\n",
    "    \\qquad \\mathcal{A}^J_j = f \\left( X^J_j, Y^J_j \\right)\n",
    "  \\end{equation*}\n",
    "\n",
    "- We can estimate $\\mathcal{A}$ from\n",
    "  \n",
    "  \\begin{equation*}\n",
    "    \\mathcal{A} \\simeq \\frac{1}{M} \\sum_{i=1}^M \\mathcal{A}^J_i = \\overline{\\mathcal{A}}\n",
    "  \\end{equation*}\n",
    "  \n",
    "- The standard deviation for $\\mathcal{A}$ is obtained from\n",
    "\n",
    "  \\begin{equation*}\n",
    "    \\sigma_\\mathcal{A} \\simeq \\sqrt{ \\frac{M-1}{M}\n",
    "    \\sum_{i=1}^M (\\mathcal{A}_i - \\overline{\\mathcal{A}})^2}\n",
    "  \\end{equation*}\n",
    "  \n",
    "- In practice, one typically uses $M \\sim 20$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "- Some care has to be taken when dealing with correlated data.\n",
    "\n",
    "- The standard deviation for a sample average can be estimated by\n",
    "\n",
    "    - Binning analysis\n",
    "  \n",
    "    - Computation of the autocorrelation function\n",
    "  \n",
    "- Estimators for quantities that are functions of averages are typically\n",
    "  biased.\n",
    "  \n",
    "- The error bar for sample estimates of a quantity $\\mathcal{A}$ can be obtained\n",
    "  either from standard error propagation. But it is more convenient to\n",
    "  use a Jackknife analysis.\n",
    "  \n",
    "- The Jackknife analysis is an example of a *resampling technique*. There are\n",
    "  other ones, especially the *bootstrap*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Outline\n",
    "    \n",
    "- [Introduction to Monte Carlo](01-intro_monte_carlo.slides.html)\n",
    "\n",
    "- [Newton–Cotes quadrature](02-newton_cotes.slides.html)\n",
    "  \n",
    "- [Importance sampling](03-importance_sampling.slides.html)\n",
    "\n",
    "- [Direct sampling methods](04-direct_sampling.slides.html)\n",
    "\n",
    "- [Markov chain sampling and balance condition](05-markov_chain.slides.html)\n",
    "\n",
    "- [Metropolis-Hastings algorithm](06-metropolis_hastings.slides.html)\n",
    "\n",
    "- [The two-dimensional Ising model](07-ising.slides.html)\n",
    "\n",
    "- [Error bar analysis](08-error_analysis.slides.html)\n",
    "  \n",
    "- [References](09-references.slides.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
